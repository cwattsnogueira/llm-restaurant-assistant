{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOS3gcZvQ8tKHaCbbHSLCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwattsnogueira/llm-restaurant-assistant/blob/main/LLM_PoweredRestaurantAssistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  LLM-Powered Restaurant Assistant  \n",
        "**Prompt Engineering + NLP + Evaluation Frameworks**  \n",
        "*By Carllos Watts-Nogueira*"
      ],
      "metadata": {
        "id": "QdxOS2si7PEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Project Overview\n",
        "\n",
        "This project demonstrates how to build and evaluate a prompt-based AI assistant for restaurant operations. It includes:\n",
        "\n",
        "- Prompt engineering for classification and Q&A\n",
        "- LLM integration with OpenAI GPT-3.5\n",
        "- Static evaluation logic and reproducibility\n",
        "- Gradio interface for real-time interaction\n",
        "- Dashboard for prompt performance and training simulation\n",
        "- Ready for deployment on Hugging Face Spaces or Streamlit"
      ],
      "metadata": {
        "id": "OfO0ICXt7WC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "kzEDDrVU9QzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required package\n",
        "!pip install openai gradio pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NeiDrcZ9Tm2",
        "outputId": "8d57b156-824d-4301-a615-e533ccd59276"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.44.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.34.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import openai\n",
        "import pandas as pd\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "xdrv_hCS9Vma"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Load your API key securely from Colab's userdata\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "oxNlTGC19WOa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Use Case 1: Order Classification"
      ],
      "metadata": {
        "id": "EHDe1jgD7hZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Goal  \n",
        "Classify incoming customer messages into predefined order categories (e.g., dine-in, takeout, delivery, reservation)."
      ],
      "metadata": {
        "id": "QtO4ltYy7kyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Prompt Template  "
      ],
      "metadata": {
        "id": "y8qEI3RK7osQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KgTfUscj6_Kt"
      },
      "outputs": [],
      "source": [
        "def classify_order_prompt(message):\n",
        "    return f\"\"\"\n",
        "You are a restaurant assistant. Classify the following message into one of the categories:\n",
        "[Dine-In, Takeout, Delivery, Reservation, Other]\n",
        "\n",
        "Message: \"{message}\"\n",
        "\n",
        "Category:\n",
        "\"\"\"\n",
        "\n",
        "def training_qa_prompt(question):\n",
        "    return f\"\"\"\n",
        "You are a restaurant training assistant. Answer the following question clearly and concisely.\n",
        "\n",
        "Question: \"{question}\"\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###   LLM Response Function\n",
        "\n"
      ],
      "metadata": {
        "id": "ylkSYwPg75Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(prompt):\n",
        "    try:\n",
        "        # Create OpenAI client instance\n",
        "        client = openai.OpenAI()\n",
        "\n",
        "        # Send prompt to GPT-3.5\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        # Return the model's response\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle and return any errors\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "h4qfx4gA79Tr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Static Evaluation Logic"
      ],
      "metadata": {
        "id": "IUaO53pi8IAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(predicted, expected):\n",
        "    return predicted.lower() == expected.lower()\n",
        "\n",
        "test_cases = [\n",
        "    {\"message\": \"I'd like to book a table for four at 7pm tonight.\", \"expected\": \"Reservation\"},\n",
        "    {\"message\": \"Can I get two burgers delivered to 5th Avenue?\", \"expected\": \"Delivery\"},\n",
        "    {\"message\": \"I'll pick up my order in 20 minutes.\", \"expected\": \"Takeout\"},\n",
        "    {\"message\": \"We’re dining in tonight, please prepare a table.\", \"expected\": \"Dine-In\"},\n",
        "    {\"message\": \"Do you have vegan options?\", \"expected\": \"Other\"},\n",
        "]\n",
        "\n",
        "def run_evaluation():\n",
        "    results = []\n",
        "    for case in test_cases:\n",
        "        prompt = classify_order_prompt(case[\"message\"])\n",
        "        prediction = get_llm_response(prompt)\n",
        "        match = evaluate_classification(prediction, case[\"expected\"])\n",
        "        results.append({\n",
        "            \"Message\": case[\"message\"],\n",
        "            \"Prediction\": prediction,\n",
        "            \"Expected\": case[\"expected\"],\n",
        "            \"Match\": match\n",
        "        })\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "CcnUb1t39s-6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Case 2: Staff Training Q&A – Sample Questions"
      ],
      "metadata": {
        "id": "0TIyxxSp95pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_questions = [\n",
        "    \"How do I handle a customer complaint?\",\n",
        "    \"What’s the protocol for food safety in the kitchen?\",\n",
        "    \"How do I process a refund?\"\n",
        "]\n",
        "\n",
        "def run_training_samples():\n",
        "    responses = []\n",
        "    for q in sample_questions:\n",
        "        prompt = training_qa_prompt(q)\n",
        "        answer = get_llm_response(prompt)\n",
        "        responses.append({\"Question\": q, \"Answer\": answer})\n",
        "    return pd.DataFrame(responses)"
      ],
      "metadata": {
        "id": "-fNayKN998hM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradio Interface"
      ],
      "metadata": {
        "id": "JGnqoXNq-FVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_order(message):\n",
        "    prompt = classify_order_prompt(message)\n",
        "    return get_llm_response(prompt)\n",
        "\n",
        "def training_answer(question):\n",
        "    prompt = training_qa_prompt(question)\n",
        "    return get_llm_response(prompt)\n",
        "\n",
        "def show_classification_dashboard():\n",
        "    return run_evaluation()\n",
        "\n",
        "def show_training_dashboard():\n",
        "    return run_training_samples()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"#  Restaurant Assistant – Powered by GPT-3.5\")\n",
        "\n",
        "    with gr.Tab(\"Order Classification\"):\n",
        "        gr.Markdown(\"Classify customer messages into order types.\")\n",
        "        msg_input = gr.Textbox(label=\"Customer Message\")\n",
        "        msg_output = gr.Textbox(label=\"Predicted Category\")\n",
        "        msg_button = gr.Button(\"Classify\")\n",
        "        msg_button.click(fn=classify_order, inputs=msg_input, outputs=msg_output)\n",
        "\n",
        "    with gr.Tab(\"Staff Training Q&A\"):\n",
        "        gr.Markdown(\"Ask training-related questions for restaurant staff.\")\n",
        "        qa_input = gr.Textbox(label=\"Training Question\")\n",
        "        qa_output = gr.Textbox(label=\"Answer\")\n",
        "        qa_button = gr.Button(\"Ask\")\n",
        "        qa_button.click(fn=training_answer, inputs=qa_input, outputs=qa_output)\n",
        "\n",
        "    with gr.Tab(\"Evaluation Dashboard\"):\n",
        "        gr.Markdown(\"Static evaluation of prompt accuracy on test cases.\")\n",
        "        dashboard_output = gr.Dataframe()\n",
        "        dashboard_button = gr.Button(\"Run Classification Evaluation\")\n",
        "        dashboard_button.click(fn=show_classification_dashboard, inputs=[], outputs=dashboard_output)\n",
        "\n",
        "    with gr.Tab(\"Training Q&A Samples\"):\n",
        "        gr.Markdown(\"Sample answers to common staff training questions.\")\n",
        "        training_output = gr.Dataframe()\n",
        "        training_button = gr.Button(\"Run Sample Q&A\")\n",
        "        training_button.click(fn=show_training_dashboard, inputs=[], outputs=training_output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "tfrfj5j--E0q",
        "outputId": "69f3ab03-c154-4e58-8c8c-bfc4685f24c0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://734469af7bb9c4bf90.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://734469af7bb9c4bf90.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Report"
      ],
      "metadata": {
        "id": "33Fv-q0pMRFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author:** Carllos Watts-Nogueira  \n",
        "**Date:** September 2025  \n",
        "**Platform:** Google Colab + Gradio + OpenAI GPT-3.5  \n",
        "**Deployment Ready:** Hugging Face Spaces / Streamlit\n",
        "\n",
        "---\n",
        "\n",
        "##  Project Objective\n",
        "\n",
        "Design and evaluate a modular AI assistant for restaurant operations using LLMs. The assistant supports two core use cases:\n",
        "\n",
        "1. **Order Classification** – Categorize customer messages into predefined order types  \n",
        "2. **Staff Training Q&A** – Answer operational questions for restaurant staff clearly and concisely\n",
        "\n",
        "---\n",
        "\n",
        "##  Technologies Used\n",
        "\n",
        "| Component         | Stack / Tool                  |\n",
        "|------------------|-------------------------------|\n",
        "| LLM Backend       | OpenAI GPT-3.5 (via API)       |\n",
        "| Interface         | Gradio (interactive UI)        |\n",
        "| Evaluation        | Pandas (static test cases)     |\n",
        "| Hosting (optional)| Hugging Face Spaces / Colab    |\n",
        "| Prompt Design     | Modular Python functions        |\n",
        "\n",
        "---\n",
        "\n",
        "##  System Architecture\n",
        "\n",
        "- **Prompt Templates**: Modular functions for each use case  \n",
        "- **LLM Integration**: OpenAI client with secure API key handling via `google.colab.userdata.get()`  \n",
        "- **Gradio UI**: Four tabs for interaction and evaluation  \n",
        "- **Evaluation Logic**: Static test cases for classification accuracy  \n",
        "- **Training Samples**: Predefined questions to simulate onboarding scenarios\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "| Message | Expected | Predicted | Match |\n",
        "|--------|----------|-----------|-------|\n",
        "| \"I'd like to book a table for four at 7pm tonight.\" | Reservation | Reservation | True  \n",
        "| \"Can I get two burgers delivered to 5th Avenue?\" | Delivery | Delivery |  True\n",
        "| \"Do you have vegan options?\" | Other | Other |   True\n",
        "| ... | ... | ... | ... |\n",
        "\n",
        "**Accuracy:** 100% on static test set (5 cases)  \n",
        "**Model Behavior:** Consistent, deterministic (temperature = 0.3)\n",
        "\n",
        "---\n",
        "\n",
        "##  Sample Training Q&A\n",
        "\n",
        "| Question | Answer |\n",
        "|---------|--------|\n",
        "| How do I handle a customer complaint? | Apologize, listen actively, offer a resolution, and escalate if needed.  \n",
        "| What’s the protocol for food safety in the kitchen? | Wash hands, sanitize surfaces, store food at correct temperatures, and follow hygiene guidelines.  \n",
        "| How do I process a refund? | Use the POS system, confirm the transaction, and follow manager approval protocols.  \n",
        "\n",
        "---\n",
        "\n",
        "##  Key Achievements\n",
        "\n",
        "- Built a fully interactive LLM assistant with real-time classification and Q&A  \n",
        "- Designed reproducible evaluation logic for prompt testing  \n",
        "- Integrated secure API key handling via Colab secrets  \n",
        "- Ready for deployment on Hugging Face Spaces or Streamlit  \n",
        "- Demonstrated prompt engineering, UI design, and model evaluation in one cohesive project\n",
        "\n",
        "---\n",
        "\n",
        "##  Next Steps\n",
        "\n",
        "- Add LangChain or Hugging Face model comparison  \n",
        "- Integrate logging and analytics (LangFuse-style)  \n",
        "- Expand training Q&A with dynamic feedback or quiz scoring  \n",
        "- Deploy permanently and track usage metrics"
      ],
      "metadata": {
        "id": "kXKx2TilNAPw"
      }
    }
  ]
}